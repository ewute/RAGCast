{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f42a9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24534162",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.allmusicals.com/e/epicthemusical.htm\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# ðŸ”¹ Step 1: find the section containing all the song links\n",
    "lyric_section = soup.find(class_=\"lyrics-list\")\n",
    "\n",
    "ol = lyric_section.find(\"ol\") # where the actual links are\n",
    "\n",
    "# ðŸ”¹ Step 2: extract <a> tags within that section\n",
    "song_links = []\n",
    "current_act = None\n",
    "\n",
    "# Iterate over direct <li> children of <ol>\n",
    "for li in ol.find_all(\"li\", recursive = False):\n",
    "    classes = li.get(\"class\", [])\n",
    "\n",
    "    # Act header like <li class=\"act\"><strong><span>Act I</span></strong></li>\n",
    "    if \"act\" in classes:\n",
    "        current_act = li.get_text(\" \", strip=True) or None\n",
    "        print(current_act)\n",
    "        continue\n",
    "\n",
    "    for a in lyric_section.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        abs_url = urljoin(url, href)\n",
    "        title = a.get_text(strip=True)\n",
    "        # Optional: only keep Epic: The Musical song pages\n",
    "        if re.search(r\"/epicthemusical/.+\\.htm$\", abs_url, re.I):\n",
    "            song_links.append((title, abs_url, current_act))\n",
    "\n",
    "# ðŸ”¹ Step 3: remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_links = []\n",
    "for title, link, act in song_links:\n",
    "    if link not in seen:\n",
    "        unique_links.append((title, link, act))\n",
    "        seen.add(link)\n",
    "\n",
    "# ðŸ”¹ Step 4: print or save the result\n",
    "# print(f\"Found {len(unique_links)} songs:\")\n",
    "#for t, u, a in unique_links:\n",
    "    #print(f\"- {a} {t} -> {u}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90fb011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Step 1: find the section containing all the song links\n",
    "lyric_section = soup.find(\"section\", class_=\"lyrics-list\")\n",
    "ol = lyric_section.find(\"ol\")  # where the actual links are\n",
    "\n",
    "# ðŸ”¹ Step 2: extract links, tracking the current act label\n",
    "song_links = []\n",
    "current_act = None\n",
    "\n",
    "for li in ol.find_all(\"li\", recursive=False):\n",
    "    classes = li.get(\"class\", [])\n",
    "\n",
    "    # Act header like <li class=\"act\">Act I</li>\n",
    "    if \"act\" in classes:\n",
    "        current_act = li.get_text(\" \", strip=True) or None\n",
    "        # print(current_act)\n",
    "        continue\n",
    "\n",
    "    # âœ… FIXED: search anchors inside THIS li\n",
    "    for a in li.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        abs_url = urljoin(url, href)\n",
    "        title = a.get_text(strip=True)\n",
    "        if re.search(r\"/epicthemusical/.+\\.htm$\", abs_url, re.I):\n",
    "            song_links.append((title, abs_url, current_act))\n",
    "\n",
    "# ðŸ”¹ De-dup\n",
    "seen = set()\n",
    "unique_links = []\n",
    "for title, link, act in song_links:\n",
    "    key = link.lower()\n",
    "    if key not in seen:\n",
    "        unique_links.append((title, link, act))\n",
    "        seen.add(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84734b",
   "metadata": {},
   "source": [
    "## Getting Song Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "add37be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_LINE = re.compile(r'^\\s*\\[(.+?)\\]\\s*$')   # e.g., [ODYSSEUS, CREW]\n",
    "\n",
    "def split_speakers(s: str):\n",
    "    \"\"\"\n",
    "    Turn 'ODYSSEUS, CREW & NARRATOR' into ['ODYSSEUS','CREW','NARRATOR'].\n",
    "    \"\"\"\n",
    "    # normalize separators\n",
    "    s = re.sub(r'\\s*(?:,|&|and|/|\\+)\\s*', ',', s, flags=re.I)\n",
    "    parts = [p.strip() for p in s.split(',') if p.strip()]\n",
    "    return parts or [\"UNKNOWN\"]\n",
    "\n",
    "def parse_song_page(title: str, url: str, act: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame with columns: song, speaker, line\n",
    "    (one row per speaker per line).\n",
    "    \"\"\"\n",
    "    r = requests.get(url, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # ---- locate the content ----\n",
    "    page = soup.find(id=\"page\")\n",
    "    if not page:\n",
    "        raise ValueError(\"Could not find <div id='page'> on this page.\")\n",
    "    \n",
    "    # song title (prefer the printed h2; fall back to <title>)\n",
    "    h2 = page.find(\"h2\")\n",
    "    song_title = title;\n",
    "    #song_title = (h2.get_text(strip=True) if h2\n",
    "    #              else (soup.title.get_text(strip=True) if soup.title else \"Unknown Song\"))\n",
    "\n",
    "    # Get the page text with explicit line breaks at <br>\n",
    "    text = page.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    rows = []\n",
    "    current_speakers = [\"UNKNOWN\"]\n",
    "    prev_speaker_groups = []   # <- keep a list of past speaker lists\n",
    "    for raw in text.splitlines():\n",
    "        line = raw.strip()\n",
    "\n",
    "        # ignore boilerplate/metadata lines\n",
    "        if not line or line.lower().startswith(\"last update\"):\n",
    "            continue\n",
    "\n",
    "        # speaker header like [ODYSSEUS, CREW]\n",
    "        m = SPEAKER_LINE.match(line)\n",
    "        if m:\n",
    "            speaker_text = m.group(1).strip()\n",
    "            # Handle special [BOTH] keyword\n",
    "            if speaker_text.lower() == \"both\":\n",
    "                flat_prev = [s for grp in prev_speaker_groups[-2:] for s in grp]\n",
    "                current_speakers = list(dict.fromkeys(flat_prev)) or [\"UNKNOWN\"]\n",
    "            else:\n",
    "                current_speakers = split_speakers(speaker_text)\n",
    "            prev_speaker_groups.append(current_speakers)\n",
    "            continue\n",
    "\n",
    "        # otherwise it's a lyric line â†’ one row per speaker\n",
    "        for spk in current_speakers:\n",
    "            rows.append({\"act\": act, \"song\": song_title, \"speaker\": spk, \"line\": line})\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06223470",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running Data Collection\n",
    "dfs = [] # apparently its faster to create a list of dataframes with the lines from each song and concat at end\n",
    "for t, u, a in unique_links:\n",
    "    df = parse_song_page(t, u, a) #url for each song\n",
    "    dfs.append(df)\n",
    "    # print(df.head(11))\n",
    "df_overall = pd.concat(dfs, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5776b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca2bd5",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall[\"speaker\"] = df_overall[\"speaker\"].str.lower()  # make all speakers lowercase\n",
    "df_overall = df_overall[df_overall[\"speaker\"] != \"unknown\"]\n",
    "df_overall = df_overall[df_overall[\"speaker\"] != \"spoken\"] #sometimes the label is [Oddesues, spoken]\n",
    "# or easier way:\n",
    "#drop_speakers = [\"UNKNOWN\", \"spoken\"]\n",
    "# df_overall = df_overall[~df_overall[\"speaker\"].isin(drop_speakers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['odysseus' 'soldiers' 'zeus' 'ensemble' 'all' 'crew' 'eurylochus'\n",
      " 'polites' 'odyssseus' 'lotus eaters' 'athena' 'polyphemus' 'soldier'\n",
      " 'cyclopes' 'perimedes' 'elpenor' 'aeolus' 'winions' 'penelope'\n",
      " 'telemachus' 'poseidon' 'laestrygonians' 'circe' 'hermes'\n",
      " 'fallen soldiers' 'tiresias' 'sirens' 'siren' 'scylla' 'antinuous'\n",
      " 'the suitors' 'telemahcus' 'calypso' 'apollo' 'hephaestus' 'aphrodite'\n",
      " 'ares' 'hera' 'suitors' 'amphinomus']\n"
     ]
    }
   ],
   "source": [
    "# Testing Data Clean Up\n",
    "print(df_overall[\"speaker\"].unique())\n",
    "# print(df_overall[df_overall[\"speaker\"] == \"spoken\"])\n",
    "# print(df_overall[df_overall[\"speaker\"] == \"both\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bf0dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_overall.to_csv(\"epic_all_songs_lines.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
