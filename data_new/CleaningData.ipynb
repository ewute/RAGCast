{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42a9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24534162",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.allmusicals.com/e/epicthemusical.htm\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26798694",
   "metadata": {},
   "source": [
    "# Scraping the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e4c0a",
   "metadata": {},
   "source": [
    "### Getting All Song Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010d02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the section containing all the song links\n",
    "lyric_section = soup.find(class_=\"lyrics-list\")\n",
    "\n",
    "ol = lyric_section.find(\"ol\") # where the actual links are\n",
    "\n",
    "# Extract <a> tags within that section\n",
    "song_links = []\n",
    "current_act = None\n",
    "\n",
    "# Iterate over direct <li> children of <ol>\n",
    "for li in ol.find_all(\"li\", recursive = False):\n",
    "    classes = li.get(\"class\", [])\n",
    "\n",
    "    # Act header like <li class=\"act\"><strong><span>Act I</span></strong></li>\n",
    "    if \"act\" in classes:\n",
    "        current_act = li.get_text(\" \", strip=True) or None\n",
    "        #print(current_act)\n",
    "        continue\n",
    "\n",
    "    for a in li.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        abs_url = urljoin(url, href)\n",
    "        title = a.get_text(strip=True)\n",
    "        # Optional: only keep Epic: The Musical song pages\n",
    "        if re.search(r\"/epicthemusical/.+\\.htm$\", abs_url, re.I):\n",
    "            song_links.append((title, abs_url, current_act))\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_links = []\n",
    "for title, link, act in song_links:\n",
    "    if link not in seen:\n",
    "        unique_links.append((title, link, act))\n",
    "        seen.add(link)\n",
    "\n",
    "# print(f\"Found {len(unique_links)} songs:\")\n",
    "#for t, u, a in unique_links:\n",
    "    #print(f\"- {a} {t} -> {u}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84734b",
   "metadata": {},
   "source": [
    "### Getting Song Lines From Each Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add37be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_LINE = re.compile(r'^\\s*\\[(.+?)\\]\\s*$')   # e.g., [ODYSSEUS, CREW]\n",
    "\n",
    "def split_speakers(s: str):\n",
    "    \"\"\"\n",
    "    Turn 'ODYSSEUS, CREW & NARRATOR' into ['ODYSSEUS','CREW','NARRATOR'].\n",
    "    \"\"\"\n",
    "    # normalize separators\n",
    "    s = re.sub(r'\\s*(?:,|&|and|/|\\+)\\s*', ',', s, flags=re.I)\n",
    "    parts = [p.strip() for p in s.split(',') if p.strip()]\n",
    "    return parts or [\"UNKNOWN\"]\n",
    "\n",
    "def parse_song_page(title: str, url: str, act: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame with columns: song, speaker, line\n",
    "    (one row per speaker per line).\n",
    "    \"\"\"\n",
    "    r = requests.get(url, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # ---- locate the content ----\n",
    "    page = soup.find(id=\"page\")\n",
    "    if not page:\n",
    "        raise ValueError(\"Could not find <div id='page'> on this page.\")\n",
    "    \n",
    "    # song title (prefer the printed h2; fall back to <title>)\n",
    "    h2 = page.find(\"h2\")\n",
    "    song_title = title;\n",
    "    \n",
    "    # Get the page text with explicit line breaks at <br>\n",
    "    text = page.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    rows = []\n",
    "    current_speakers = [\"UNKNOWN\"]\n",
    "    prev_speaker_groups = []   # <- keep a list of past speaker lists\n",
    "    for raw in text.splitlines():\n",
    "        line = raw.strip()\n",
    "\n",
    "        # ignore boilerplate/metadata lines\n",
    "        if not line or line.lower().startswith(\"last update\"):\n",
    "            continue\n",
    "\n",
    "        # speaker header like [ODYSSEUS, CREW]\n",
    "        m = SPEAKER_LINE.match(line)\n",
    "        if m:\n",
    "            speaker_text = m.group(1).strip()\n",
    "            # Handle special [BOTH] keyword\n",
    "            if speaker_text.lower() == \"both\":\n",
    "                flat_prev = [s for grp in prev_speaker_groups[-2:] for s in grp]\n",
    "                current_speakers = list(dict.fromkeys(flat_prev)) or [\"UNKNOWN\"]\n",
    "            else:\n",
    "                current_speakers = split_speakers(speaker_text)\n",
    "            prev_speaker_groups.append(current_speakers)\n",
    "            continue\n",
    "\n",
    "        # otherwise it's a lyric line → one row per speaker\n",
    "        for spk in current_speakers:\n",
    "            rows.append({\"act\": act, \"song\": song_title, \"speaker\": spk, \"line\": line})\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06223470",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running Data Collection\n",
    "dfs = [] # apparently its faster to create a list of dataframes with the lines from each song and concat at end\n",
    "for t, u, a in unique_links:\n",
    "    df = parse_song_page(t, u, a) #url for each song\n",
    "    dfs.append(df)\n",
    "    # print(df.head(11))\n",
    "df_overall = pd.concat(dfs, ignore_index = True)\n",
    "#df_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e0aebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNKNOWN' 'ODYSSEUS' 'SOLDIERS' 'ZEUS' 'ENSEMBLE' 'ALL' 'CREW'\n",
      " 'EURYLOCHUS' 'POLITES' 'ODYSSSEUS' 'LOTUS EATERS' 'ATHENA' 'spoken'\n",
      " 'POLYPHEMUS' 'SOLDIER' 'CYCLOPES' 'PERIMEDES' 'ELPENOR' 'AEOLUS'\n",
      " 'WINIONS' 'PENELOPE' 'TELEMACHUS' 'POSEIDON' 'LAESTRYGONIANS' 'CIRCE'\n",
      " 'HERMES' 'FALLEN SOLDIERS' 'TIRESIAS' 'SIRENS' 'SIREN' 'SCYLLA'\n",
      " 'ANTINUOUS' 'THE SUITORS' 'TELEMAHCUS' 'CALYPSO' 'APOLLO' 'HEPHAESTUS'\n",
      " 'APHRODITE' 'ARES' 'HERA' 'Poseidon' 'SUITORS' 'AMPHINOMUS']\n"
     ]
    }
   ],
   "source": [
    "len(df_overall)\n",
    "print(df_overall[\"speaker\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca2bd5",
   "metadata": {},
   "source": [
    "# Clean Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2966bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall[\"speaker\"] = df_overall[\"speaker\"].str.lower()  # make all speakers lowercase\n",
    "df_overall = df_overall[df_overall[\"speaker\"] != \"unknown\"]\n",
    "df_overall = df_overall[df_overall[\"speaker\"] != \"spoken\"] #sometimes the label is [Oddesues, spoken]\n",
    "corrections = { #all the corrections that need t obe made\n",
    "    \"odyssseus\": \"odysseus\",\n",
    "    \"telemahcus\": \"telemachus\",\n",
    "    \"the suitors\": \"suitors\"\n",
    "}\n",
    "df_overall[\"speaker\"] = df_overall[\"speaker\"].replace(corrections)\n",
    "df_overall['speaker'] = df_overall['speaker'].str.title() #capitalize the names of speakers\n",
    "# or easier way:\n",
    "#drop_speakers = [\"UNKNOWN\", \"spoken\"]\n",
    "# df_overall = df_overall[~df_overall[\"speaker\"].isin(drop_speakers)]\n",
    "\n",
    "\n",
    "counts = df_overall[\"speaker\"].value_counts()\n",
    "# print(f\"Number of unique speakers: {df_overall[\"speaker\"].nunique()}\")\n",
    "# print(counts)\n",
    "\n",
    "# Testing Data Clean Up\n",
    "#print(df_overall[\"speaker\"].unique())\n",
    "# print(df_overall[df_overall[\"speaker\"] == \"spoken\"])\n",
    "#print(df_overall[df_overall[\"speaker\"] == \"odyssseus\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02fe8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers kept: 33\n"
     ]
    }
   ],
   "source": [
    "# Drop speakers with less than 5 lines to avoid tiny classes\n",
    "min_lines = 5\n",
    "keep_speakers = counts[counts >= min_lines].index\n",
    "df_finaldata = df_overall[df_overall[\"speaker\"].isin(keep_speakers)].reset_index(drop=True)\n",
    "print(\"Speakers kept:\", len(keep_speakers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf0dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with all speakers present\n",
    "df_overall.to_csv(\"epic_all_songs_lines_allspeakers.csv\", index=False, encoding=\"utf-8\")\n",
    "# CSV with only the speakers we want\n",
    "df_finaldata.to_csv(\"epic_all_songs_lines_trainingdata.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020bf4f",
   "metadata": {},
   "source": [
    "## Creating a Document for Each Speaker (as opposed to CSV with all lines in box above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef9dc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## Train/Test Split, stratified by speaker\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_finaldata[\"line\"], df_finaldata[\"speaker\"],\n",
    "    test_size=0.2, random_state=42, stratify=df_finaldata[\"speaker\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94114cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [train] Aeolus: 30 lines written\n",
      "✅ [train] All: 32 lines written\n",
      "✅ [train] Antinuous: 66 lines written\n",
      "✅ [train] Aphrodite: 5 lines written\n",
      "✅ [train] Apollo: 4 lines written\n",
      "✅ [train] Ares: 8 lines written\n",
      "✅ [train] Athena: 84 lines written\n",
      "✅ [train] Calypso: 71 lines written\n",
      "✅ [train] Circe: 57 lines written\n",
      "✅ [train] Crew: 44 lines written\n",
      "✅ [train] Cyclopes: 4 lines written\n",
      "✅ [train] Ensemble: 87 lines written\n",
      "✅ [train] Eurylochus: 76 lines written\n",
      "✅ [train] Fallen Soldiers: 4 lines written\n",
      "✅ [train] Hephaestus: 4 lines written\n",
      "✅ [train] Hera: 7 lines written\n",
      "✅ [train] Hermes: 76 lines written\n",
      "✅ [train] Laestrygonians: 33 lines written\n",
      "✅ [train] Lotus Eaters: 7 lines written\n",
      "✅ [train] Odysseus: 575 lines written\n",
      "✅ [train] Penelope: 69 lines written\n",
      "✅ [train] Perimedes: 4 lines written\n",
      "✅ [train] Polites: 39 lines written\n",
      "✅ [train] Polyphemus: 24 lines written\n",
      "✅ [train] Poseidon: 88 lines written\n",
      "✅ [train] Scylla: 7 lines written\n",
      "✅ [train] Sirens: 4 lines written\n",
      "✅ [train] Soldiers: 144 lines written\n",
      "✅ [train] Suitors: 66 lines written\n",
      "✅ [train] Telemachus: 55 lines written\n",
      "✅ [train] Tiresias: 10 lines written\n",
      "✅ [train] Winions: 19 lines written\n",
      "✅ [train] Zeus: 46 lines written\n",
      "✅ [test] Aeolus: 7 lines written\n",
      "✅ [test] All: 8 lines written\n",
      "✅ [test] Antinuous: 17 lines written\n",
      "✅ [test] Aphrodite: 1 lines written\n",
      "✅ [test] Apollo: 1 lines written\n",
      "✅ [test] Ares: 2 lines written\n",
      "✅ [test] Athena: 21 lines written\n",
      "✅ [test] Calypso: 18 lines written\n",
      "✅ [test] Circe: 14 lines written\n",
      "✅ [test] Crew: 11 lines written\n",
      "✅ [test] Cyclopes: 1 lines written\n",
      "✅ [test] Ensemble: 22 lines written\n",
      "✅ [test] Eurylochus: 19 lines written\n",
      "✅ [test] Fallen Soldiers: 1 lines written\n",
      "✅ [test] Hephaestus: 1 lines written\n",
      "✅ [test] Hera: 1 lines written\n",
      "✅ [test] Hermes: 19 lines written\n",
      "✅ [test] Laestrygonians: 8 lines written\n",
      "✅ [test] Lotus Eaters: 2 lines written\n",
      "✅ [test] Odysseus: 144 lines written\n",
      "✅ [test] Penelope: 17 lines written\n",
      "✅ [test] Perimedes: 1 lines written\n",
      "✅ [test] Polites: 10 lines written\n",
      "✅ [test] Polyphemus: 6 lines written\n",
      "✅ [test] Poseidon: 22 lines written\n",
      "✅ [test] Scylla: 2 lines written\n",
      "✅ [test] Sirens: 1 lines written\n",
      "✅ [test] Soldiers: 36 lines written\n",
      "✅ [test] Suitors: 17 lines written\n",
      "✅ [test] Telemachus: 14 lines written\n",
      "✅ [test] Tiresias: 3 lines written\n",
      "✅ [test] Winions: 5 lines written\n",
      "✅ [test] Zeus: 11 lines written\n"
     ]
    }
   ],
   "source": [
    "## Split into test and train data\n",
    "# 1️⃣ Rebuild train/test DataFrames from the split\n",
    "train_df = pd.DataFrame({\n",
    "    \"speaker\": y_train.values,\n",
    "    \"line\": X_train.values\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"speaker\": y_test.values,\n",
    "    \"line\": X_test.values\n",
    "})\n",
    "\n",
    "# 2️⃣ Base folder for all speaker text files\n",
    "base_dir = \"speaker_texts\"\n",
    "splits = {\n",
    "    \"train\": train_df,\n",
    "    \"test\": test_df,\n",
    "}\n",
    "\n",
    "for split_name, split_df in splits.items():\n",
    "    split_dir = os.path.join(base_dir, split_name)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    # Group by speaker and write one file per speaker\n",
    "    for speaker, group in split_df.groupby(\"speaker\"):\n",
    "        lines = group[\"line\"].dropna().tolist()\n",
    "        text = \"\\n\".join(lines)\n",
    "\n",
    "        # Sanitize filename a bit\n",
    "        filename = (\n",
    "            str(speaker)\n",
    "            .replace(\"/\", \"_\")\n",
    "            .replace(\"\\\\\", \"_\")\n",
    "            .replace(\" \", \"_\")\n",
    "        )\n",
    "        path = os.path.join(split_dir, f\"{filename}.txt\")\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "        print(f\"✅ [{split_name}] {speaker}: {len(lines)} lines written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cca1db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Aeolus: 37 lines written\n",
      "✅ All: 40 lines written\n",
      "✅ Antinuous: 83 lines written\n",
      "✅ Aphrodite: 6 lines written\n",
      "✅ Apollo: 5 lines written\n",
      "✅ Ares: 10 lines written\n",
      "✅ Athena: 105 lines written\n",
      "✅ Calypso: 89 lines written\n",
      "✅ Circe: 71 lines written\n",
      "✅ Crew: 55 lines written\n",
      "✅ Cyclopes: 5 lines written\n",
      "✅ Ensemble: 109 lines written\n",
      "✅ Eurylochus: 95 lines written\n",
      "✅ Fallen Soldiers: 5 lines written\n",
      "✅ Hephaestus: 5 lines written\n",
      "✅ Hera: 8 lines written\n",
      "✅ Hermes: 95 lines written\n",
      "✅ Laestrygonians: 41 lines written\n",
      "✅ Lotus Eaters: 9 lines written\n",
      "✅ Odysseus: 719 lines written\n",
      "✅ Penelope: 86 lines written\n",
      "✅ Perimedes: 5 lines written\n",
      "✅ Polites: 49 lines written\n",
      "✅ Polyphemus: 30 lines written\n",
      "✅ Poseidon: 110 lines written\n",
      "✅ Scylla: 9 lines written\n",
      "✅ Sirens: 5 lines written\n",
      "✅ Soldiers: 180 lines written\n",
      "✅ Suitors: 83 lines written\n",
      "✅ Telemachus: 69 lines written\n",
      "✅ Tiresias: 13 lines written\n",
      "✅ Winions: 24 lines written\n",
      "✅ Zeus: 57 lines written\n"
     ]
    }
   ],
   "source": [
    "# Make a folder to hold all character text files\n",
    "os.makedirs(\"speaker_texts\", exist_ok=True)\n",
    "\n",
    "# Group lines by speaker and save each one\n",
    "for speaker, group in df_finaldata.groupby(\"speaker\"):\n",
    "    lines = group[\"line\"].dropna().tolist()\n",
    "    text = \"\\n\".join(lines)\n",
    "    \n",
    "    # Sanitize file name (remove slashes/spaces)\n",
    "    filename = speaker.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "    path = os.path.join(\"speaker_texts\", f\"{filename}.txt\")\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"✅ {speaker}: {len(lines)} lines written\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
